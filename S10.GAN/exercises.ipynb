{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "insert some Batch Normalization layers into the generator and discriminator networks and see if it helps training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 16:58:37.626899: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# load the mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalize the data\n",
    "x_train = x_train.astype('float32') / 255 *2 -1\n",
    "x_test = x_test.astype('float32') / 255 *2 -1\n",
    "\n",
    "# reshape the data\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takamatsunaoki/opt/anaconda3/envs/ai-master/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_learn = 10001 # number of learning\n",
    "interval = 1000 # interval of learning\n",
    "batch_size = 32 # batch size\n",
    "n_noize = 128 # number of noize\n",
    "img_size = 28 # image size\n",
    "alpha = 0.2 # alpha for leaky relu\n",
    "\n",
    "# set up the optimizer\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "Activation in hidden layer:  LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:02:28.794670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 784)               803600    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,493,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "\n",
    "# generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_dim=n_noize))\n",
    "generator.add(LeakyReLU(alpha=alpha))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=alpha))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(alpha=alpha))\n",
    "generator.add(Dense(img_size*img_size, activation='tanh'))\n",
    "generator.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "Activation in hidden layer:  LeakyReLU\n",
    "loss:  binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# discriminator\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(512, input_dim=img_size*img_size))\n",
    "discriminator.add(LeakyReLU(alpha=alpha))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(alpha=alpha))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.summary()\n",
    "\n",
    "# compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 784)               1493520   \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1)                 533505    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,027,025\n",
      "Trainable params: 1,493,520\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# training the discriminator only when training the GAN\n",
    "discriminator.trainable = False\n",
    "\n",
    "# the discriminator determines the image created by the generator for the noise\n",
    "noize = Input(shape=(n_noize,))\n",
    "img = generator(noize)\n",
    "reality = discriminator(img)\n",
    "\n",
    "# combine the generator and the discriminator\n",
    "comb = Model(noize, reality)\n",
    "comb.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "comb.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def gerenate_img(i):\n",
    "    # generate the image\n",
    "    n_row = 5\n",
    "    n_col = 5\n",
    "    noize = np.random.normal(0, 1, (n_row*n_col, n_noize))\n",
    "    gen_imgs = generator.predict(noize)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    matrix_image = np.zeros((img_size*n_row, img_size*n_col))\n",
    "\n",
    "    # arrange the generated images into a single image\n",
    "    for j in range(n_row):\n",
    "        for k in range(n_col):\n",
    "            matrix_image[j*img_size:(j+1)*img_size, k*img_size:(k+1)*img_size] = gen_imgs[j*n_col+k].reshape(img_size, img_size)\n",
    "    \n",
    "    # show the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(matrix_image, cmap='Greys_r')\n",
    "    plt.tick_params(labelbottom=False, labelleft=False, labelright=False, labeltop=False)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the learning\n",
    "batch_half = int(batch_size / 2)\n",
    "loss_record = np.zeros((n_learn, 3))\n",
    "acc_record = np.zeros((n_learn, 2))\n",
    "\n",
    "# learning\n",
    "for i in range(n_learn):\n",
    "    # train the discriminator with the generated images\n",
    "    g_noize = np.random.normal(0, 1, (batch_half, n_noize))\n",
    "    g_imgs = generator.predict(g_noize)\n",
    "    loss_fake, acc_fake = discriminator.train_on_batch(g_imgs, np.zeros((batch_half, 1)))\n",
    "    loss_record[i, 0] = loss_fake\n",
    "    acc_record[i, 0] = acc_fake\n",
    "\n",
    "    # train the discriminator with the real images\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_half)\n",
    "    real_imgs = x_train[idx, :]\n",
    "    loss_real, acc_real = discriminator.train_on_batch(real_imgs, np.ones((batch_half, 1)))\n",
    "    loss_record[i, 1] = loss_real\n",
    "    acc_record[i, 1] = acc_real\n",
    "\n",
    "    # train the generator withe the combined model\n",
    "    c_noize = np.random.normal(0, 1, (batch_size, n_noize))\n",
    "    loss_comb = comb.train_on_batch(c_noize, np.ones((batch_size, 1)))\n",
    "    loss_record[i, 2] = loss_comb\n",
    "\n",
    "    # show the learning process\n",
    "    if i % interval == 0:\n",
    "        print('epoch: %d, loss_fake: %f, loss_real: %f, loss_comb: %f, acc_fake: %f, acc_real: %f' % (i, loss_fake, loss_real, loss_comb, acc_fake, acc_real))\n",
    "        gerenate_img(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History of the loss and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history of the loss\n",
    "n_plt_loss = 500\n",
    "plt.plot(np.arange(n_plt_loss), loss_record[:n_plt_loss, 0], label='loss_fake')\n",
    "plt.plot(np.arange(n_plt_loss), loss_record[:n_plt_loss, 1], label='loss_real')\n",
    "plt.plot(np.arange(n_plt_loss), loss_record[:n_plt_loss, 2], label='loss_comb')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# history of the accuracy\n",
    "n_plt_acc = 1000\n",
    "plt.plot(np.arange(n_plt_acc), acc_record[:n_plt_acc, 0], label='acc_fake')\n",
    "plt.plot(np.arange(n_plt_acc), acc_record[:n_plt_acc, 1], label='acc_real')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2512312fceb6e908b1c6087f9c29f0bb0c93a332146a69690954edd67c2954cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
