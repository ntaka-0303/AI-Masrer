{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "obukri2_dAfw"
      },
      "source": [
        "# Exercises\n",
        "Introduce Experience Replay and Fixed Target Q-Networks into the DQN algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AXX8265dAfy"
      },
      "source": [
        "## Imports and setup optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "omP0qjX1dAfz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import animation, rc\n",
        "from  dataclasses import dataclass, field\n",
        "from typing import List\n",
        "import collections\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, ReLU\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# set up the optimizer\n",
        "optimizer = RMSprop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhQ8av3YdAf5"
      },
      "source": [
        "## Brain class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yO-H_t93dAf5"
      },
      "outputs": [],
      "source": [
        "class Brain:\n",
        "    # initialize the brain of the agent with the following parameters\n",
        "    # build the model\n",
        "    def __init__(self, n_states, n_hidden, n_actions, gamma=0.9, r=0.99):\n",
        "        self.epsilon = 1.0 # initial exploration rate\n",
        "        self.gamma = gamma # discount factor\n",
        "        self.r = r # decay rate of epsilon\n",
        "\n",
        "        self.experiences = collections.deque(maxlen=10000) # store the experiences\n",
        "        self.batch_size = 32 # mini-batch size\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_hidden, input_shape=(n_states,)))\n",
        "        model.add(ReLU())\n",
        "        model.add(Dense(n_hidden))\n",
        "        model.add(ReLU())\n",
        "        model.add(Dense(n_actions))\n",
        "        model.compile(loss='mse', optimizer=optimizer)\n",
        "        self.model = model\n",
        "\n",
        "    # train the model with the bellman equation using the experience replay\n",
        "    def train(self):\n",
        "        batch_size = min(self.batch_size, len(self.experiences))\n",
        "\n",
        "        # randomly sample the experiences for the mini-batch\n",
        "        batch = np.random.choice(self.experiences, batch_size)\n",
        "        states_batch = np.array([e.state for e in batch])\n",
        "        next_states_batch = np.array([e.next_state for e in batch])\n",
        "        actions_batch = np.array([e.action for e in batch])\n",
        "        rewards_batch = np.array([e.reward for e in batch])\n",
        "        terminal_batch = np.array([e.terminal for e in batch])\n",
        "\n",
        "        # target is the reward + discounted Q value of the next state\n",
        "        q = self.model.predict(states_batch)\n",
        "        q_next = self.model.predict(next_states_batch, verbose=0)\n",
        "        t = np.copy(q)\n",
        "\n",
        "        # target is the reward + discounted Q value of the next state\n",
        "        for i in range(batch_size):\n",
        "            if terminal_batch[i]:\n",
        "                t[i, actions_batch[i]] = rewards_batch[i]\n",
        "            else:\n",
        "                t[i, actions_batch[i]] = rewards_batch[i] + self.gamma * np.max(q_next[i])\n",
        "        \n",
        "        # train the model\n",
        "        self._train_on_batch(states_batch, t)\n",
        "\n",
        "    # _train_on_batch is a wrapper for the keras train_on_batch function\n",
        "    @tf.function\n",
        "    def _train_on_batch(self, states_batch, t):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # get the loss\n",
        "            loss = self.model.loss(t, self.model(states_batch))\n",
        "        \n",
        "        # get the gradients\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        # update the weights\n",
        "        optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "\n",
        "    # get the action with the epsilon-greedy policy\n",
        "    def get_action(self, state):\n",
        "        q = self.model.predict(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action =  np.random.randint(q.shape[1], size=q.shape[0])\n",
        "        else:\n",
        "            action = np.argmax(q, axis=1)\n",
        "        if self.epsilon > 0.1: # decay the exploration rate\n",
        "            self.epsilon *= self.r\n",
        "        return action\n",
        "    \n",
        "    # store the experience\n",
        "    def store_experience(self, state, next_state, action, reward, terminal):\n",
        "        self.experiences.append(Experience(state, next_state, action, reward, terminal))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: np.ndarray = np.array([[]])\n",
        "    next_state: np.ndarray = np.array([[]])\n",
        "    action: List[int] = field(default_factory=list)\n",
        "    reward: int = 0\n",
        "    terminal: bool = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDGKXpnadAf6"
      },
      "source": [
        "## Agent class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pbmRLS0EdAf7"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    # initialize the agent with the following action parameters\n",
        "    def __init__(self, v_x, v_y_sigma, v_jump, brain, target_brain):\n",
        "        self.v_x = v_x # velocity in the x direction\n",
        "        self.v_y_sigma = v_y_sigma # std of the velocity in the y direction\n",
        "        self.v_jump = v_jump # velocity in the y direction when the agent jumps\n",
        "        self.brain = brain # the brain of the agent\n",
        "        self.target_brain = target_brain # the target brain of the agent\n",
        "        self.reset()\n",
        "\n",
        "    # reset the agent position and velocity in the y direction\n",
        "    # initial position is (x, y) = (-1, 0)\n",
        "    # initial velocity in the y direction is randomly sampled from a normal distribution\n",
        "    def reset(self):\n",
        "        self.x = -1. # x position\n",
        "        self.y = 0. # y position\n",
        "        self.v_y = self.v_y_sigma * np.random.randn() # y velocity\n",
        "\n",
        "    # update the target brain\n",
        "    def update_target_brain(self):\n",
        "        self.target_brain.model.set_weights(self.brain.model.get_weights())\n",
        "\n",
        "    # update the agent steate, reward, and brain\n",
        "    # the reward is 0 if the agent position is y = -1 or y = 1\n",
        "    # the reward is 1 if the agent position is x = 1\n",
        "    # if the agent jumps, the velocity in the y direction is updated by the jump velocity\n",
        "    # if the agent does not jump, the velocity in the y direction is updated by the gravity\n",
        "    def step(self, g):\n",
        "        # get the current state\n",
        "        states = np.array([[self.y, self.v_y]])\n",
        "\n",
        "        # update the agent position and get the reward and terminal\n",
        "        self.x += self.v_x\n",
        "        self.y += self.v_y\n",
        "        reward = 0\n",
        "        terminal = False \n",
        "        if self.y < -1.0 or self.y > 1.0:\n",
        "            reward = -1\n",
        "            terminal = True\n",
        "        elif self.x > 1.0:\n",
        "            reward = 1\n",
        "            terminal = True\n",
        "        reward = np.array([reward])\n",
        "        \n",
        "        # get the action from the target brain and get the next state\n",
        "        action = self.target_brain.get_action(states)\n",
        "        if action[0] == 0:\n",
        "            self.v_y -= g\n",
        "        else:\n",
        "            self.v_y = self.v_jump\n",
        "        next_states = np.array([[self.y, self.v_y]])\n",
        "\n",
        "        # store the experience for the brain\n",
        "        self.brain.store_experience(states, next_states, action, reward, terminal)\n",
        "        \n",
        "        # train the brain\n",
        "        self.brain.train()\n",
        "\n",
        "        # if the episode is terminated, reset the agent\n",
        "        if terminal:\n",
        "            self.reset()\n",
        "\n",
        "        return reward, terminal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QSeP3zmdAf-"
      },
      "source": [
        "## Environment class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ucyofBTbdAf-"
      },
      "outputs": [],
      "source": [
        "class Enviroment:\n",
        "    # initialize the enviroment with the following parameters\n",
        "    def __init__(self, g):\n",
        "        self.g = g\n",
        "\n",
        "    # advance the agent by one step\n",
        "    def step(self):\n",
        "        return self.agent.step(self.g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfLxI6_2dAf_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0EFNGuB8dAf_"
      },
      "outputs": [],
      "source": [
        "# train the agent and show the animation\n",
        "# build the q-net brain and target brain\n",
        "n_states = 2 # number of states\n",
        "n_hidden = 32 # number of hidden units\n",
        "n_actions = 2 # number of actions\n",
        "q_net_brain = Brain(n_states, n_hidden, n_actions, r=0.99)\n",
        "target_brain = Brain(n_states, n_hidden, n_actions, r=0.99)\n",
        "\n",
        "# build the agent\n",
        "v_x = 0.05 # velocity in the x direction\n",
        "v_y_sigma = 0.1 # std of the velocity in the y direction\n",
        "v_jump = 0.2 # velocity in the y direction when the agent jumps\n",
        "agent = Agent(v_x, v_y_sigma, v_jump, q_net_brain, target_brain)\n",
        "\n",
        "# build the enviroment\n",
        "g = 0.2 # gravity\n",
        "enviroment = Enviroment(agent, g)\n",
        "\n",
        "# episode parameters\n",
        "n_episodes = 1000 # number of episodes\n",
        "terminal = False\n",
        "reward_histroy = []\n",
        "step_history = []\n",
        "for episord in range(n_episodes):\n",
        "    step = 0\n",
        "    while not terminal:\n",
        "        step += 1\n",
        "        # advance the enviroment by one step\n",
        "        reward, terminal = enviroment.step()\n",
        "\n",
        "        # end the step if terminal\n",
        "        if terminal:\n",
        "            step_history.append(step)\n",
        "            reward_histroy.append(reward)\n",
        "            # the target brain updated every 100 steps\n",
        "            if step % 100 == 0:\n",
        "                agent.update_target_brain()\n",
        "            break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.arrange(len(step_history))\n",
        "\n",
        "# show the reward history\n",
        "plt.plot(x, reward_histroy)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.show()\n",
        "\n",
        "# show the step history\n",
        "plt.plot(x, step_history)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Step')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ai-master",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2512312fceb6e908b1c6087f9c29f0bb0c93a332146a69690954edd67c2954cb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
