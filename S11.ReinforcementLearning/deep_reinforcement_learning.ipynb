{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforment Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 17:41:52.733336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, ReLU\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# set up the optimizer\n",
    "optimizer = RMSprop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    # initialize the brain of the agent with the following parameters\n",
    "    # build the model\n",
    "    def __init__(self, n_states, n_hidden, n_actions, gamma=0.9, r=0.99):\n",
    "        self.epsilon = 1.0 # initial exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.r = r # decay rate of epsilon\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(n_hidden, input_shape=(n_states,)))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dense(n_hidden))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dense(n_actions))\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        self.model = model\n",
    "\n",
    "    # train the model with predicted Q values\n",
    "    # if the episode is terminated, the target is the reward\n",
    "    # if the episode is not terminated, the target is the reward + discounted Q value of the next state\n",
    "    def train(self, states, next_states, action ,reward, terminal):\n",
    "        q = self.model.predict(states)\n",
    "        q_next = self.model.predict(next_states)\n",
    "        t = np.copy(q)\n",
    "        if terminal:\n",
    "            t[:, action] = reward # if the episode is terminated, the target is the reward\n",
    "        else:\n",
    "            t[:, action] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "        self.model.fit(states, t, verbose=0)\n",
    "\n",
    "    # get the action with the highest Q value\n",
    "    # if the random number is less than the exploration rate, the action is random\n",
    "    # if the random number is greater than the exploration rate, the action is the one with the highest Q value\n",
    "    def get_action(self, state):\n",
    "        q = self.model.predict(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action =  np.random.randint(0, q.shape[1])\n",
    "        else:\n",
    "            action = np.argmax(q, axis=1)\n",
    "        if self.epsilon > 0.1: # decay the exploration rate\n",
    "            self.epsilon *= self.r\n",
    "        return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # initialize the agent with the following action parameters\n",
    "    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n",
    "        self.v_x = v_x # velocity in the x direction\n",
    "        self.v_y_sigma = v_y_sigma # std of the velocity in the y direction\n",
    "        self.v_jump = v_jump # velocity in the y direction when the agent jumps\n",
    "        self.brain = brain\n",
    "        self.reset()\n",
    "\n",
    "    # reset the agent position and velocity in the y direction\n",
    "    # initial position is (x, y) = (-1, 0)\n",
    "    # initial velocity in the y direction is randomly sampled from a normal distribution\n",
    "    def reset(self):\n",
    "        self.x = -1. # x position\n",
    "        self.y = 0. # y position\n",
    "        self.v_y = self.v_y_sigma * np.random.randn() # y velocity\n",
    "\n",
    "    # update the agent steate, reward, and brain\n",
    "    # the reward is 0 if the agent position is y = -1 or y = 1\n",
    "    # the reward is 1 if the agent position is x = 1\n",
    "    # if the agent jumps, the velocity in the y direction is updated by the jump velocity\n",
    "    # if the agent does not jump, the velocity in the y direction is updated by the gravity\n",
    "    def step(self, g):\n",
    "        states = np.array([[self.y, self.v_y]])\n",
    "        self.x += self.v_x\n",
    "        self.y += self.v_y\n",
    "\n",
    "        reward = 0\n",
    "        terminal = False \n",
    "        # set the reward and terminal flag\n",
    "        if self.y < -1 or self.y > 1:\n",
    "            reward = -1\n",
    "            terminal = True\n",
    "        elif self.x > 1:\n",
    "            reward = 1\n",
    "            terminal = True\n",
    "        reward = np.array([reward])\n",
    "\n",
    "        # get the action from the brain\n",
    "        action = self.brain.get_action(states)\n",
    "        if action[0] == 1:\n",
    "            self.v_y = self.v_jump\n",
    "        else:\n",
    "            self.v_y -= g\n",
    "        \n",
    "        # update the brain\n",
    "        next_states = np.array([[self.y, self.v_y]])\n",
    "        self.brain.train(states, next_states, action, reward, terminal)\n",
    "\n",
    "        # if the episode is terminated, reset the agent\n",
    "        if terminal:\n",
    "            self.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enviroment:\n",
    "    # initialize the enviroment with the following parameters\n",
    "    def __init__(self, agent, g):\n",
    "        self.agent = agent\n",
    "        self.g = g\n",
    "\n",
    "    # advance the agent by one step\n",
    "    def step(self):\n",
    "        self.agent.step(self.g)\n",
    "        return self.agent.x, self.agent.y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the animation of the agent action\n",
    "def animation(enviroment , interval , frames):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.close() # close the figure\n",
    "    ax.set_xlim((-1, 1))\n",
    "    ax.set_ylim((-1, 1))\n",
    "    sc = ax.scatter([], [], s=100, c='r')\n",
    "\n",
    "    # plot the agent position\n",
    "    def plot(data):\n",
    "        x, y = enviroment.step()\n",
    "        sc.set_offsets(np.array[x, y])\n",
    "        return sc,\n",
    "\n",
    "    return animation.FuncAnimation(fig, plot, frames=frames, interval=interval, blit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the agent and show the animation\n",
    "def main(r=0.99):\n",
    "    # build the brain\n",
    "    n_states = 2 # number of states\n",
    "    n_hidden = 32 # number of hidden units\n",
    "    n_actions = 2 # number of actions\n",
    "    brain = Brain(n_states, n_hidden, n_actions, r=r)\n",
    "\n",
    "    # build the agent\n",
    "    v_x = 0.05 # velocity in the x direction\n",
    "    v_y_sigma = 0.1 # std of the velocity in the y direction\n",
    "    v_jump = 0.2 # velocity in the y direction when the agent jumps\n",
    "    agent = Agent(v_x, v_y_sigma, v_jump, brain)\n",
    "\n",
    "    # build the enviroment\n",
    "    g = 0.2 # gravity\n",
    "    enviroment = Enviroment(agent, g)\n",
    "\n",
    "    # show the animation\n",
    "    interval = 50 # interval between frames\n",
    "    frames = 1024 # number of frames\n",
    "    ani = animation(enviroment, interval, frames)\n",
    "    rc('animation', html='jshtml')\n",
    "    return ani"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the agetnt acting randomly\n",
    "ani = main(r=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the agent acting with the trained brain\n",
    "ani = main(r=0.99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2512312fceb6e908b1c6087f9c29f0bb0c93a332146a69690954edd67c2954cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
