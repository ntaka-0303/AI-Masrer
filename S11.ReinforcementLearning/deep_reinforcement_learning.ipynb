{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obukri2_dAfw"
      },
      "source": [
        "# Deep Reinforment Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AXX8265dAfy"
      },
      "source": [
        "## Imports and setup optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "omP0qjX1dAfz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, ReLU\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# set up the optimizer\n",
        "optimizer = RMSprop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhQ8av3YdAf5"
      },
      "source": [
        "## Brain class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yO-H_t93dAf5"
      },
      "outputs": [],
      "source": [
        "class Brain:\n",
        "    # initialize the brain of the agent with the following parameters\n",
        "    # build the model\n",
        "    def __init__(self, n_states, n_hidden, n_actions, gamma=0.9, r=0.99):\n",
        "        self.epsilon = 1.0 # initial exploration rate\n",
        "        self.gamma = gamma # discount factor\n",
        "        self.r = r # decay rate of epsilon\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_hidden, input_shape=(n_states,)))\n",
        "        model.add(ReLU())\n",
        "        model.add(Dense(n_hidden))\n",
        "        model.add(ReLU())\n",
        "        model.add(Dense(n_actions))\n",
        "        model.compile(loss='mse', optimizer=optimizer)\n",
        "        self.model = model\n",
        "\n",
        "    # train the model with predicted Q values\n",
        "    # if the episode is terminated, the target is the reward\n",
        "    # if the episode is not terminated, the target is the reward + discounted Q value of the next state\n",
        "    def train(self, states, next_states, action ,reward, terminal):\n",
        "        q = self.model.predict(states)\n",
        "        q_next = self.model.predict(next_states)\n",
        "        t = np.copy(q)\n",
        "        if terminal:\n",
        "            t[:, action] = reward # if the episode is terminated, the target is the reward\n",
        "        else:\n",
        "            t[:, action] = reward + self.gamma * np.max(q_next, axis=1)\n",
        "        self.model.fit(states, t, verbose=0)\n",
        "\n",
        "    # get the action with the highest Q value\n",
        "    # if the random number is less than the exploration rate, the action is random\n",
        "    # if the random number is greater than the exploration rate, the action is the one with the highest Q value\n",
        "    def get_action(self, state):\n",
        "        q = self.model.predict(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action =  np.random.randint(q.shape[1], size=q.shape[0])\n",
        "        else:\n",
        "            action = np.argmax(q, axis=1)\n",
        "        if self.epsilon > 0.1: # decay the exploration rate\n",
        "            self.epsilon *= self.r\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDGKXpnadAf6"
      },
      "source": [
        "## Agent class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pbmRLS0EdAf7"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    # initialize the agent with the following action parameters\n",
        "    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n",
        "        self.v_x = v_x # velocity in the x direction\n",
        "        self.v_y_sigma = v_y_sigma # std of the velocity in the y direction\n",
        "        self.v_jump = v_jump # velocity in the y direction when the agent jumps\n",
        "        self.brain = brain\n",
        "        self.reset()\n",
        "\n",
        "    # reset the agent position and velocity in the y direction\n",
        "    # initial position is (x, y) = (-1, 0)\n",
        "    # initial velocity in the y direction is randomly sampled from a normal distribution\n",
        "    def reset(self):\n",
        "        self.x = -1. # x position\n",
        "        self.y = 0. # y position\n",
        "        self.v_y = self.v_y_sigma * np.random.randn() # y velocity\n",
        "\n",
        "    # update the agent steate, reward, and brain\n",
        "    # the reward is 0 if the agent position is y = -1 or y = 1\n",
        "    # the reward is 1 if the agent position is x = 1\n",
        "    # if the agent jumps, the velocity in the y direction is updated by the jump velocity\n",
        "    # if the agent does not jump, the velocity in the y direction is updated by the gravity\n",
        "    def step(self, g):\n",
        "        states = np.array([[self.y, self.v_y]])\n",
        "        self.x += self.v_x\n",
        "        self.y += self.v_y\n",
        "\n",
        "        reward = 0\n",
        "        terminal = False \n",
        "        # set the reward and terminal flag\n",
        "        if self.y < -1 or self.y > 1:\n",
        "            reward = -1\n",
        "            terminal = True\n",
        "        elif self.x > 1:\n",
        "            reward = 1\n",
        "            terminal = True\n",
        "        reward = np.array([reward])\n",
        "\n",
        "        # get the action from the brain\n",
        "        action = self.brain.get_action(states)\n",
        "        if action[0] == 1:\n",
        "            self.v_y = self.v_jump\n",
        "        else:\n",
        "            self.v_y -= g\n",
        "        \n",
        "        # update the brain\n",
        "        next_states = np.array([[self.y, self.v_y]])\n",
        "        self.brain.train(states, next_states, action, reward, terminal)\n",
        "\n",
        "        # if the episode is terminated, reset the agent\n",
        "        if terminal:\n",
        "            self.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QSeP3zmdAf-"
      },
      "source": [
        "## Environment class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ucyofBTbdAf-"
      },
      "outputs": [],
      "source": [
        "class Enviroment:\n",
        "    # initialize the enviroment with the following parameters\n",
        "    def __init__(self, agent, g):\n",
        "        self.agent = agent\n",
        "        self.g = g\n",
        "\n",
        "    # advance the agent by one step\n",
        "    def step(self):\n",
        "        self.agent.step(self.g)\n",
        "        return self.agent.x, self.agent.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW73O_70dAf-"
      },
      "source": [
        "## Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aZPcd93QdAf_"
      },
      "outputs": [],
      "source": [
        "# show the animation of the agent action\n",
        "def animate(enviroment, interval, frames):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.close()\n",
        "    ax.set_xlim(( -1, 1))\n",
        "    ax.set_ylim((-1, 1))\n",
        "    sc = ax.scatter([], [])\n",
        "\n",
        "    def plot(data):\n",
        "        x, y = enviroment.step()\n",
        "        sc.set_offsets(np.array([[x, y]]))\n",
        "        return (sc,)\n",
        "\n",
        "    return animation.FuncAnimation(fig, plot, interval=interval, frames=frames, blit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfLxI6_2dAf_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0EFNGuB8dAf_"
      },
      "outputs": [],
      "source": [
        "# train the agent and show the animation\n",
        "def main(r=0.99):\n",
        "    # build the brain\n",
        "    n_states = 2 # number of states\n",
        "    n_hidden = 32 # number of hidden units\n",
        "    n_actions = 2 # number of actions\n",
        "    brain = Brain(n_states, n_hidden, n_actions, r=r)\n",
        "\n",
        "    # build the agent\n",
        "    v_x = 0.05 # velocity in the x direction\n",
        "    v_y_sigma = 0.1 # std of the velocity in the y direction\n",
        "    v_jump = 0.2 # velocity in the y direction when the agent jumps\n",
        "    agent = Agent(v_x, v_y_sigma, v_jump, brain)\n",
        "\n",
        "    # build the enviroment\n",
        "    g = 0.2 # gravity\n",
        "    enviroment = Enviroment(agent, g)\n",
        "\n",
        "    # show the animation\n",
        "    interval = 50 # interval between frames\n",
        "    frames = 1024 # number of frames\n",
        "    ani = animate(enviroment, interval, frames)\n",
        "    rc('animation', html='jshtml')\n",
        "    ani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6nTGFUPdAgA"
      },
      "source": [
        "## Random action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PqY-fB8UdAgA",
        "outputId": "48c9edc8-13fb-4c51-d6b3-5bc4c4dbd0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ],
      "source": [
        "# observe the agetnt acting randomly\n",
        "main(r=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnra8DSZdAgB"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qtbAPwxEdAgB",
        "outputId": "29ae8df7-2294-4553-d017-39bf83d96cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n"
          ]
        }
      ],
      "source": [
        "# observe the agent acting with the trained brain\n",
        "ani = main(r=0.99)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-master",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2512312fceb6e908b1c6087f9c29f0bb0c93a332146a69690954edd67c2954cb"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}