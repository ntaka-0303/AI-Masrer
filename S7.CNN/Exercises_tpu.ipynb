{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfObWTKEiA4x"
      },
      "source": [
        "# Exerciese\n",
        "Improbe the model and try to get the accuracy above 80%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbXTykAgiA6F"
      },
      "source": [
        "## Load the data and Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8HfKYaDfiA6G"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Normalize the data.\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyox3fS8iA6I"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model() :\n",
        "    # Define the model.\n",
        "    model = tf.keras.layers.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "thKrPQ56mC1r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH-rChNuiA6K"
      },
      "source": [
        "## Compile the model useing TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wtvVHk_-iA6O",
        "outputId": "25e920a2-78e8-4bf8-927b-74ebb29cfdd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0cf67b73df95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Set the strategy for computing usin TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtpu_grpc_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"grpc://\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"COLAB_TPU_ADDR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtpu_cluster_resolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_grpc_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUDistributionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_cluster_resolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ],
      "source": [
        "# Set the parameters for the model.\n",
        "batch_size = 500\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "init_lr = 0.001\n",
        "\n",
        "# Set the opimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=init_lr)\n",
        "\n",
        "# Set the strategy for computing usin TPU\n",
        "import os\n",
        "tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n",
        "strategy = tf.distribute.TPUDistributionStrategy(tpu_cluster_resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbjAXU6xiA6Q"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJFDkq0OiA6Q"
      },
      "outputs": [],
      "source": [
        "# Set the early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# Set the Learning rate schedule\n",
        "def exponential_decay(epoch, lr):\n",
        "    decay_rate = 0.95\n",
        "    decay_steps = epochs\n",
        "    new_lr = lr * (decay_rate ** (epoch / decay_steps))\n",
        "    return new_lr\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay)\n",
        "\n",
        "# Create ImageDataGenerator objects for data augmentation.\n",
        "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "generator.fit(x_train)\n",
        "\n",
        "history = model.fit(generator.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(x_train) / batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    validation_steps=len(x_test) / batch_size,\n",
        "                    callbacks=[lr_scheduler, early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pObkZWRiA6R"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk18eSjJiA6S"
      },
      "outputs": [],
      "source": [
        "# Plot the loss and accuracy curves for training and validation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(np.arange(len(train_loss)), train_loss, label='train_loss')\n",
        "plt.plot(np.arange(len(val_loss)), val_loss, label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(len(train_acc)), train_acc, label='train_accuracy')\n",
        "plt.plot(np.arange(len(val_acc)), val_acc, label='val_accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-master",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.16 (default, Jan 17 2023, 09:28:58) \n[Clang 14.0.6 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2512312fceb6e908b1c6087f9c29f0bb0c93a332146a69690954edd67c2954cb"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}